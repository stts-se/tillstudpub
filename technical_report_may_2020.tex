% -*- latex -*-

\documentclass[11pt, a4paper, twoside]{article}
\usepackage{a4wide}
%\usepackage{a4}
%--------------------- Swedish ------------------------
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc} %% Latin-1 / ISO-8859-1
% \usepackage[utf8]{inputenc} %% UTF-8
%\usepackage[swedish]{babel}
%-------------------------------------------------------

%% For att kunna ange totalt antal sidor, anvand paketet lastpage
%% nedan, samt efter \begin{document} lagg till:
%% \cfoot{\thepage (\pageref{LastPage})}
\usepackage{lastpage} 

%% \numberwithin{figure}{section}
%% \numberwithin{table}{section}
%% \setlength{\parindent}{0 pt}
%% \setlength{\parskip}{6 pt}

%% if you need to change page margins:
%% \setlength{\topmargin}{-1cm}
%% \setlength{\textheight}{24cm}

% Unix:
% Ctrl-c-f to make a dvi file of this
% Ctrl-c-v to open it in XDVI

\usepackage{fancybox}

\usepackage{fancyhdr}

\makeatletter
\renewcommand{\subsubsection}{\@startsection
  {subsubsection}%
  {3}%
  {0mm}%
  {-\baselineskip}%
  {0.5\baselineskip}%
  {\bfseries\sffamily}}%
\renewcommand{\subsection}{\@startsection
  {subsection}%
  {2}%
  {0mm}%
  {-\baselineskip}%
  {0.5\baselineskip}%
  {\bfseries\sffamily\large}}%
\renewcommand{\section}{\@startsection
  {section}%
  {1}%
  {0mm}%
  {-\baselineskip}%
  {0.5\baselineskip}%
  {\bfseries\sffamily\Large}}%
\makeatother


\pagestyle{fancy}
\fancyhf{}

\newcommand{\stts}{STTS S\"oder{\-}malms tal{\-}teknologi{\-}service}

\lhead{\small \sc \stts }

\rhead{\small stts.se}


%\usepackage{amssymb}
\title{ Microphone capture in the web browser \\ \em -- draft -- }


\author{ \stts \\
  Folkungagatan 122 2tr, 116 30 Stockholm\\
  http://stts.se }

%-------------------------------------------------------
%---------------------- DOCUMENT -----------------------
%-------------------------------------------------------

\begin{document}

\maketitle
%  \cfoot{\thepage}
\cfoot{\thepage (\pageref{LastPage})}  

\tableofcontents

\newpage

\section{Introduction}

In this document, we describe the background for some demo
applications using the web audio API for microphone recording in the
browser.  Different methods for recording in the browser and sending
the resulting audio to a server are listed below. 

There is audio processing built into the browser, and there is a
JavaScript web audio API (but the audio processing itself is probably not
implemented in JavaScript).

There are different ways to capture audio in the browser, and
different ways to transporting the audio to a server.

This document is accompanied by some example applications at
\begin{quote}
  \tt http://github.com/stts-se/tillstudpub    
\end{quote}



These consist of small client-server
examples, run in the browser, from which the audio is sent to a server
and saved to disk. All examples work in recent versions of Chrome, but
may not work in other browsers.

Precompiled versions for Linux and Windows are found at 
\begin{quote}
\tt http://github.com/stts-se/tillstudpub/releases
\end{quote}



\begin{enumerate}
\item {\tt promptrec} presents the user with text to read aloud. The
  recordings are saved on the server.

\item {\tt audio\_streaming} streams microphone audio from the browser
  to a server. There are two competing methods, one
  using { \tt ScriptProcessorNode}, the other using {\tt
    AudioWorklet}, a more recent approach.

\item {\tt webrtc\_demo} is a sketchy example of using the WebRTC
  protocol for streaming audio to a server. Not a fully functioning
  application, and based on an example of the Pion WebRTC implementation.
  See pion.ly and github.com/pion.
  
\end{enumerate}




\section{Recording a complete file before sending to the server}

One way to do microphone recording in a web browser is using the
JavaScript Web Audio API to create an audio file in the browser. You
can obtain an audio stream from the microphone using the {\tt
  getUserMedia()} function. Using a {\tt MediaRecorder}, the audio can
be saved as a {\tt Blob}, ``a file-like object of immutable, raw
data'' \cite{blob}. When the recording is done, the {\tt Blob} can be
added to an HTML5 audio element, that can be played using the web
browser.

The audio file can be sent to a HTTP server (typically as a base64 encoded
string), and saved on the server.

The audio files might be of different encodings, depending on
the browser, so you need to pass his information on to the server
(unless you convert to a common format in the browser, but this may be
non-trivial).

This method works when the user wants to record something, and only
when done send it to the server. This could, for example, be a
recording tool, where you read aloud and record a manuscript sentence
presented in the browser, such as the current {\tt promptrec} demo.

However, this method is not useful for streaming audio to the server.

\subsection*{Pros}
\begin{itemize}
\item The audio format is known
\item A complete audio file is created before sending it over the network
\end{itemize}

\subsection*{Cons}
\begin{itemize}
\item The audio format cannot be changed (it is controlled by the browser)
\item Not for streaming
\end{itemize}


\section{Network streaming}

In the example applications, two methods are used for streaming: WebSocket for the {\tt audio\_streaming} application, and a simple demo of WebRTC.

\subsection{WebSocket}

When a client calls an HTTP server, there is a request from the
client, and a response from the server, and that's it. Subsequent
calls must establish new connections to the server.

A WebSocket, on the other hand, is a TCP connection that may stay open
for any period of time, and on which both the client and the server
may send data. A WebSocket connection is created by upgrading from an
HTTP connection, and it also has much of the same characteristics,
such as guarantees against package loss, etc.

After a WebSocket connection has been established, the
client and the server may send text or binary data to each other in
any form.

A difference between a WebSocket and an HTTP connection, is that
there is not a well defined protocol for interaction between client
and server (such as GET and POST, etc). You have to create your own
protocol. 

\subsection{WebRTC}


%https://developer.mozilla.org/en-US/docs/Web/Guide/API/WebRTC/Peer-to-peer_communications_with_WebRTC



WebRTC (Web Real-Time Communication) \cite{webrtc} is a peer-to-peer method for
streaming audio and video. In other words, you can use it to stream
audio and video directly between web-browsers (as long as there is
some way of the browsers to find each other, for example using a STUN
server).

WebRTC is build on UDP (rather than HTTP). The WebRTC uses the Opus
codec for audio (and V8 for video).

Since human cognition is more forgiving to missing samples than to
latency, as little lag as possible is more important than being sure
that the original sound wave is complete and intact --- packet loss is
tolerated. The original sound wave might not be possible to reproduce
on the other end. Since speech is full of redundant information and
human cognition is built for interpreting a noisy signal, this may not
be a big issue for understanding speech sent over a bad network using
WebRTC.

Under the hood, WebRTC takes care of things like echo-canceling and
noise reduction. 


WebRTC also includes a DataChannel API, that can be used for
transporting data without packet loss. However, a WebSocket may do the
job equally good, if you do not need peer-to-peer capabilities.

\subsection*{Pros}
\begin{itemize}
\item Supported by most browsers \cite{webrtcsupport}
\end{itemize}

\subsection*{Cons}
\begin{itemize}
\item Risk of packet loss
\item Little risk of latency
\end{itemize}


\section{Web Audio API processing}

The Web Audio API makes it possible to do advanced audio processing in the web browser.

In the {\tt audio\_streaming} demo, it is used to access the audio stream, convert to int16, and send over the network continuously.

\subsection{ScriptProcessorNode}

The ScriptProcessorNode\cite{scriptprocessornode} was introduced to
meet developers' need to write custom code for audio processing in the
Web Audio API. This is an audio processing module that gives access to
an input and an output buffer. Between these, you can put your own code
to do audio processing of the audio data stream.

For performance, most processing components of the Web
Audio API are run in a separate thread, but the ScriptProcessorNode is
run in the main thread, which can cause delays. It has since been
deprecated. More information on the motivation behind the move to
AudioWorklet can be found in \cite{icmc}.

In the example application, the audio data are converted from floats
to 16 bit integers in the browser.

\subsection*{Pros}
\begin{itemize}
\item Supported by most browsers
\end{itemize}

\subsection*{Cons}
\begin{itemize}
\item Can have performance and quality issues
\item Deprecated
\end{itemize}


\subsection{AudioWorklet}

The AudioWorklet was developed to cope shortcomings of its
predecessor, ScriptProcessorNode. The first implementation of the
AudioWorklet was released 2018 for Chrome \cite{icmc}.

The AudioWorklet works much in the same way as the
ScriptProcessorNode, in that it takes an input and output stream, in
between which you can insert audio processing.


Unfortunately, the AudioWorklet is yet not fully supported by Firefox,
but according to the documentation, it should work with Firefox
upcoming version 76. We have it tested and working with Google Chrome
(v81) and Opera (v68).

In the example application, the audio data are converted from floats
to 16 bit integers in the browser.

\subsection*{Pros}
\begin{itemize}
\item Processing run in a separate thread
\item Less latency than ScriptProcessorNode
\end{itemize}

\subsection*{Cons}
\begin{itemize}
\item Not supported by all browsers (see \cite{audioworklet})
\end{itemize}




\section{Browser settings for audio and streaming}

There seems to be some settings in the browser(s), that are difficult to control. Examples:

\begin{description}
\item[Sample rate] Can be retrieved but not changed
\item[Channel count] The demo applications are intended for mono
\item[Audio encoding]\ \\[-14pt]
  \begin{description}
  \item[\em{AudioWorklet/ScriptProcessorNode}] For streaming, a browser's recording is by definition 32bit PCM. We have chosen to convert to 16bit PCM before sending to server.
  \item[\em{WebRTC}] OPUS
  \item[\em{ Non-streaming}] The browser controls the encoding, but the used encoding can be retrieved.
    
  \end{description}

\end{description}


\section{WebAssembly}

A side note: WebAssembly (Wasm) can be used instead of JavaScript to
write browser audio software. The benefits may include better
performance, and being able to program in general purpose languages
that compiles to WebAssembly, instead of JavaScript. (This makes it
easy to use the same code on both client and server.)

A drawback is that WebAssembly is relatively new, and that it can be
more complicated to get running than just writing a piece of JavaScript.



\begin{thebibliography}{9}

\bibitem{highperfnetworking}
  \textit{High Performance Browser Networking},
  Ilya Grigorik,
  O'Reilly,
  2013
  
\bibitem{scriptprocessornode}
  https://developer.mozilla.org/en-US/docs/Web/API/ScriptProcessorNode

\bibitem{audioworklet}
  https://developer.mozilla.org/en-US/docs/Web/API/AudioWorklet

\bibitem{icmc}
  \textit{AudioWorklet: The future of web audio},
  Hongchan Choi,
  Google Chrome,
  ICMC,
  2018,
  https://hoch.io/media/icmc-2018-choi-audioworklet.pdf

\bibitem{webaudioapi}
  Boris Smus,
  \textit{Web Audio API},
  O'Reilly,
  2013

\bibitem{webrtc}
  https://webrtc.org/
  
\bibitem{webrtcsupport}
  https://en.wikipedia.org/wiki/WebRTC\#Support
  
\bibitem{}
  \textit{Investigation into low latency live video streaming performance of WebRTC},\newline
  Jakob Tidestr\"om,
  In degree project computer science and engineering, second cycle, \newline
  Stockholm,
  Sweden,
  2019,\newline
  http://www.diva-portal.se/smash/get/diva2:1304486/FULLTEXT01.pdf


\bibitem{blob}
  https://developer.mozilla.org/en-US/docs/Web/API/Blob
  
\end{thebibliography}

\end{document}


